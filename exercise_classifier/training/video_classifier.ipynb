{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8nzeNbN6ovg"
      },
      "outputs": [],
      "source": [
        "# Load keypoints as csv\n",
        "import pandas as pd\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "RUbQIZgwM-5u",
        "outputId": "31a845f9-e149-462c-f0ef-adf1e54e11ae"
      },
      "outputs": [
        {
          "ename": "ParserError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-ccb740567f0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mexercise\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \"\"\"\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"full_video_dataset.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: EOF inside string starting at row 133"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Schema of the data:\n",
        "- 6 columns for the angles sequences\n",
        "- exercise name\n",
        "\"\"\"\n",
        "df = pd.read_csv(\"full_video_dataset.csv\")\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kk2e9etRGu1N"
      },
      "outputs": [],
      "source": [
        "# Encode the labels\n",
        "label_to_idx = {}\n",
        "\n",
        "idx_to_label = {}\n",
        "\n",
        "for label in df.exercise_label.unique():\n",
        "  label_to_idx[label] = len(label_to_idx)\n",
        "  idx_to_label[label_to_idx[label]] = label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWtQAPMzPcI4",
        "outputId": "39f4e2ef-3615-4862-fd6b-6bd66f34ac86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of classes 6\n"
          ]
        }
      ],
      "source": [
        "NUM_CLASSES = len(idx_to_label)\n",
        "\n",
        "print(\"Number of classes\", NUM_CLASSES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txFWAV0KXF4B",
        "outputId": "5ed66925-12c7-4d9d-972b-4c5e6c0f09ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0: 'Deadlift', 1: 'Biceps curl', 2: 'Push-up', 3: 'Tricep Pushdown', 4: 'Lat pulldown', 5: 'Incline bench press'}\n"
          ]
        }
      ],
      "source": [
        "print(idx_to_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mD9XqdeYIccs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETt4wXFd3yxt",
        "outputId": "06043fd2-ffeb-4b44-96a2-b768190051c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "216\n",
            "2\n",
            "50\n",
            "8\n",
            "216\n",
            "216\n"
          ]
        }
      ],
      "source": [
        "def frame_to_input(row):\n",
        "  # HERE\n",
        "  keypoints = row[\"right_elbow_angle\":\"left_knee_angle\"]\n",
        "\n",
        "  # result = \n",
        "  results = []\n",
        "  for keypoint in keypoints:\n",
        "    results.append(list(map(float, keypoints[0].replace(\"(\", \"\").replace(\")\", \"\").split(\",\"))))\n",
        "  # 8 angle values\n",
        "  # print(len(results))\n",
        "\n",
        "  label = label_to_idx[row[\"exercise_label\"]]\n",
        "  return [results, label]\n",
        "\n",
        "lengthes = set()\n",
        "\n",
        "dataset = df.apply(frame_to_input, axis=1)\n",
        "\n",
        "reformatted_dataset = []\n",
        "\n",
        "for data, label in dataset:\n",
        "  multivariate_timeseries = list(zip(*data))\n",
        "\n",
        "  lengthes.add(len(multivariate_timeseries))\n",
        "\n",
        "  for i in range(len(multivariate_timeseries)):\n",
        "    multivariate_timeseries[i] = list(multivariate_timeseries[i])\n",
        "\n",
        "  reformatted_dataset.append([multivariate_timeseries, label])\n",
        "\n",
        "print(len(reformatted_dataset))\n",
        "print(len(reformatted_dataset[0]))\n",
        "print(len(reformatted_dataset[0][0]))\n",
        "print(len(reformatted_dataset[0][0][0]))\n",
        "\n",
        "reformatted_data, labels = list(zip(*reformatted_dataset))\n",
        "\n",
        "print(len(reformatted_data))\n",
        "print(len(labels))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MiBdpOHoJiPI",
        "outputId": "a59ed4c9-83af-420c-c7cc-7c0dd4c23e3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "216\n"
          ]
        }
      ],
      "source": [
        "print(len(dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tVAjdysI6Zt"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader , Dataset\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, dataset, labels):\n",
        "    self.dataset = dataset\n",
        "    self.labels = labels\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return torch.tensor(self.dataset[idx]), torch.tensor(self.labels[idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pt1unA869ePD",
        "outputId": "975b8efb-56be-4034-aa80-e17d5ac149be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "216\n"
          ]
        }
      ],
      "source": [
        "custom_dataset = CustomDataset(reformatted_data, labels)\n",
        "\n",
        "data_loader = DataLoader(custom_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "print(len(custom_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQ_HBmWPwK-u"
      },
      "outputs": [],
      "source": [
        "# NUM_CLASSES = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjuYEnoYBaDt"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvWJrA1C8euE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.autograd import Variable \n",
        "\n",
        "class VideoExerciseClassifier(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # self.activation = nn.Tanh()\n",
        "\n",
        "    self.num_layers = 2\n",
        "\n",
        "    self.hidden_size = 8\n",
        "    self.input_size = 50\n",
        "\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    self.lstm = nn.LSTM(8, 8, num_layers=2, batch_first=True)\n",
        "    self.dense = nn.Linear(self.hidden_size, NUM_CLASSES)\n",
        "    # self.dense_2 = nn.Linear(12, NUM_CLASSES)\n",
        "\n",
        "    \n",
        "\n",
        "  # def forward(self, input):\n",
        "  #   out = self.dense(input)\n",
        "  #   out = self.activation(out)\n",
        "  #   out = self.dense_2(out)\n",
        "\n",
        "  #   return out\n",
        "\n",
        "  def forward(self,x):\n",
        "    h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)) #hidden state\n",
        "    c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)) #internal state\n",
        "    # Propagate input through LSTM\n",
        "    output, (hn, cn) = self.lstm(x, (h_0, c_0)) #lstm with input, hidden, and internal state\n",
        "    # print(hn.size())\n",
        "    # hn = hn.view(-1, self.hidden_size) #reshaping the data for Dense layer next\n",
        "    out = self.relu(hn[-1])\n",
        "    out = self.dense(out) #first Dense\n",
        "    # out = self.relu(out) #relu\n",
        "    # out = self.fc(out) #Final Output\n",
        "    # print(out.size())\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lP8jTq_9MWq"
      },
      "outputs": [],
      "source": [
        "# training loop\n",
        "model = VideoExerciseClassifier().to(device)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.005)\n",
        "\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "def train():\n",
        "  model.train()\n",
        "\n",
        "  losses = []\n",
        "  for batch, labels in data_loader:\n",
        "    output = model(batch.to(device))\n",
        "\n",
        "    l = loss(output, labels.to(device))\n",
        "\n",
        "    losses.append(l.item())\n",
        "\n",
        "    l.backward()\n",
        "    optimizer.step()\n",
        "    model.zero_grad()\n",
        "\n",
        "  print(\"Mean Loss\", sum(losses)/len(losses))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-usFm1Z9_jf"
      },
      "outputs": [],
      "source": [
        "def frame_accuracy(logits, labels):\n",
        "  probs = nn.functional.softmax(logits, dim=1)\n",
        "  preds = torch.argmax(probs, dim=1)\n",
        "\n",
        "  # print(preds)\n",
        "\n",
        "  return sum([1 if preds[i].item() == label.item() else 0 for i, label in enumerate(labels)])/labels.size()[0]\n",
        "\n",
        "\n",
        "def evaluate():\n",
        "  model.eval()\n",
        "  accuracies = []\n",
        "  for batch, labels in data_loader:\n",
        "    output = model(batch.to(device))\n",
        "\n",
        "    accuracies.append(frame_accuracy(output, labels.to(device)))\n",
        "\n",
        "  mean = sum(accuracies)/len(accuracies)\n",
        "\n",
        "  print(\"Mean accuracy\", mean)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QM-h9dTa94Oe",
        "outputId": "601440d3-cc6a-47e4-ac2d-18dd2f5f513f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean accuracy 0.041666666666666664\n",
            "Epoch 1\n",
            "Mean Loss 1.709089113054452\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 2\n",
            "Mean Loss 1.364462550315592\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 3\n",
            "Mean Loss 1.1656380008768152\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 4\n",
            "Mean Loss 1.0613048054553844\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 5\n",
            "Mean Loss 1.0049903784637098\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 6\n",
            "Mean Loss 0.9682757210124422\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 7\n",
            "Mean Loss 0.936291566601506\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 8\n",
            "Mean Loss 0.9066841362251176\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 9\n",
            "Mean Loss 0.8820123014350733\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 10\n",
            "Mean Loss 0.8592099592917495\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 11\n",
            "Mean Loss 0.8368066741084611\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 12\n",
            "Mean Loss 0.8134984848675905\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 13\n",
            "Mean Loss 0.7904381787887326\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 14\n",
            "Mean Loss 0.7718590752532085\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 15\n",
            "Mean Loss 0.7661889087822702\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 16\n",
            "Mean Loss 0.7908693060140919\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 17\n",
            "Mean Loss 0.7885925021988375\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 18\n",
            "Mean Loss 0.6829873055081677\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 19\n",
            "Mean Loss 0.6801467946972009\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 20\n",
            "Mean Loss 0.8497203724389827\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 21\n",
            "Mean Loss 0.9259579057494799\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 22\n",
            "Mean Loss 0.9025982045740993\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 23\n",
            "Mean Loss 0.8980935225608172\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 24\n",
            "Mean Loss 0.8963213913418628\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 25\n",
            "Mean Loss 0.8959688730537891\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 26\n",
            "Mean Loss 0.8953831899497244\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 27\n",
            "Mean Loss 0.8947174096548999\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 28\n",
            "Mean Loss 0.8941688907367212\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 29\n",
            "Mean Loss 0.8939667789748421\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 30\n",
            "Mean Loss 0.8925744790445875\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 31\n",
            "Mean Loss 0.8930497980780072\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 32\n",
            "Mean Loss 0.892109126266506\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 33\n",
            "Mean Loss 0.8926444871834031\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 34\n",
            "Mean Loss 0.8922464102506638\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 35\n",
            "Mean Loss 0.8919161784428137\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 36\n",
            "Mean Loss 0.8915595624733854\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 37\n",
            "Mean Loss 0.8912359739619272\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 38\n",
            "Mean Loss 0.8910502925239228\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 39\n",
            "Mean Loss 0.8910195412183249\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 40\n",
            "Mean Loss 0.8907465308352753\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 41\n",
            "Mean Loss 0.8905063780645529\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 42\n",
            "Mean Loss 0.8901379091872109\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 43\n",
            "Mean Loss 0.8901287987828255\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 44\n",
            "Mean Loss 0.888678967124886\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 45\n",
            "Mean Loss 0.8898179768412201\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 46\n",
            "Mean Loss 0.8895439400717065\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 47\n",
            "Mean Loss 0.8892884153734755\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 48\n",
            "Mean Loss 0.8883312217615269\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 49\n",
            "Mean Loss 0.8889029132271254\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 50\n",
            "Mean Loss 0.8888082889219125\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 51\n",
            "Mean Loss 0.8885871191267614\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 52\n",
            "Mean Loss 0.8883378353935701\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 53\n",
            "Mean Loss 0.8881602127242971\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 54\n",
            "Mean Loss 0.8876090000073115\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 55\n",
            "Mean Loss 0.8877163170664398\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 56\n",
            "Mean Loss 0.8873888593580987\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 57\n",
            "Mean Loss 0.8869069983009938\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 58\n",
            "Mean Loss 0.8871278072948809\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 59\n",
            "Mean Loss 0.885890585542829\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 60\n",
            "Mean Loss 0.8866644362332644\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 61\n",
            "Mean Loss 0.8862562571410779\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 62\n",
            "Mean Loss 0.8859380024174849\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 63\n",
            "Mean Loss 0.8858273138326628\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 64\n",
            "Mean Loss 0.8854237411309172\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 65\n",
            "Mean Loss 0.8853996013877569\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 66\n",
            "Mean Loss 0.8846188097916268\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 67\n",
            "Mean Loss 0.8845594913043358\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 68\n",
            "Mean Loss 0.8842769439021746\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 69\n",
            "Mean Loss 0.8839090207108745\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 70\n",
            "Mean Loss 0.8835438960404308\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 71\n",
            "Mean Loss 0.8827496411071883\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 72\n",
            "Mean Loss 0.8826365370165419\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 73\n",
            "Mean Loss 0.8819873308142027\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 74\n",
            "Mean Loss 0.8812903734268965\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 75\n",
            "Mean Loss 0.8807477234966226\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 76\n",
            "Mean Loss 0.8798279584281974\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 77\n",
            "Mean Loss 0.8791692669468897\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 78\n",
            "Mean Loss 0.8769053562923714\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 79\n",
            "Mean Loss 0.8766092223425707\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 80\n",
            "Mean Loss 0.8747408290704092\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 81\n",
            "Mean Loss 0.8716182556969149\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 82\n",
            "Mean Loss 0.8675787037721386\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 83\n",
            "Mean Loss 0.858550422169544\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 84\n",
            "Mean Loss 0.8313105269162743\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 85\n",
            "Mean Loss 0.8339073645571867\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 86\n",
            "Mean Loss 0.8436392073829969\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 87\n",
            "Mean Loss 0.8893472701311111\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 88\n",
            "Mean Loss 0.8980316483864078\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 89\n",
            "Mean Loss 0.8935173353939144\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 90\n",
            "Mean Loss 0.8922252002413626\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 91\n",
            "Mean Loss 0.8914029109809134\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 92\n",
            "Mean Loss 0.890759924357688\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 93\n",
            "Mean Loss 0.8903330467917301\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 94\n",
            "Mean Loss 0.8899623874436926\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 95\n",
            "Mean Loss 0.8898357532366559\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 96\n",
            "Mean Loss 0.889664986067348\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 97\n",
            "Mean Loss 0.889443702581856\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 98\n",
            "Mean Loss 0.889099649809025\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 99\n",
            "Mean Loss 0.8889644587481463\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 100\n",
            "Mean Loss 0.8887872334431719\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 101\n",
            "Mean Loss 0.8886725432067005\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 102\n",
            "Mean Loss 0.8884796756837103\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 103\n",
            "Mean Loss 0.8883563153169773\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 104\n",
            "Mean Loss 0.8879670798778534\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 105\n",
            "Mean Loss 0.8880747781583557\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 106\n",
            "Mean Loss 0.887967826867545\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 107\n",
            "Mean Loss 0.8878268940305268\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 108\n",
            "Mean Loss 0.8876922911515942\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 109\n",
            "Mean Loss 0.8873607726008804\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 110\n",
            "Mean Loss 0.8874790635373857\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 111\n",
            "Mean Loss 0.887456229025567\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 112\n",
            "Mean Loss 0.887325308803055\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 113\n",
            "Mean Loss 0.8871109376626986\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 114\n",
            "Mean Loss 0.8868893401490318\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 115\n",
            "Mean Loss 0.8869598258148741\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 116\n",
            "Mean Loss 0.8868329631233657\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 117\n",
            "Mean Loss 0.8867993658339536\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 118\n",
            "Mean Loss 0.8865459703460887\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 119\n",
            "Mean Loss 0.8866258692686204\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 120\n",
            "Mean Loss 0.8864581998851564\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 121\n",
            "Mean Loss 0.8864238033140147\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 122\n",
            "Mean Loss 0.8863175481005952\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 123\n",
            "Mean Loss 0.8862318716667317\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 124\n",
            "Mean Loss 0.8857442488272985\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 125\n",
            "Mean Loss 0.8859441349351848\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 126\n",
            "Mean Loss 0.8859915175923595\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 127\n",
            "Mean Loss 0.8852784552231983\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 128\n",
            "Mean Loss 0.8857619707231168\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 129\n",
            "Mean Loss 0.8855939256372275\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 130\n",
            "Mean Loss 0.8855316582377311\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 131\n",
            "Mean Loss 0.8852737965407195\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 132\n",
            "Mean Loss 0.885217756860786\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 133\n",
            "Mean Loss 0.8849425773929667\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 134\n",
            "Mean Loss 0.885060720835571\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 135\n",
            "Mean Loss 0.8845213844820305\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 136\n",
            "Mean Loss 0.8845069935476338\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 137\n",
            "Mean Loss 0.884529230495294\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 138\n",
            "Mean Loss 0.8841603769472351\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 139\n",
            "Mean Loss 0.8841480656906411\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 140\n",
            "Mean Loss 0.8839605076721421\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 141\n",
            "Mean Loss 0.8836245819650315\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 142\n",
            "Mean Loss 0.8831846456008928\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 143\n",
            "Mean Loss 0.8820039002155816\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 144\n",
            "Mean Loss 0.8829416202174293\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 145\n",
            "Mean Loss 0.8822997975404616\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 146\n",
            "Mean Loss 0.8818168095140545\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 147\n",
            "Mean Loss 0.8813173632930826\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 148\n",
            "Mean Loss 0.8804681768847836\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 149\n",
            "Mean Loss 0.8796975346351111\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 150\n",
            "Mean Loss 0.8781361836526129\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 151\n",
            "Mean Loss 0.8759141481584973\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 152\n",
            "Mean Loss 0.8727446358512949\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 153\n",
            "Mean Loss 0.8700471089945899\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 154\n",
            "Mean Loss 0.8633072401086489\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 155\n",
            "Mean Loss 0.8528952976619756\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 156\n",
            "Mean Loss 0.8362362346163502\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 157\n",
            "Mean Loss 0.8151325753165616\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 158\n",
            "Mean Loss 0.8567750976869354\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 159\n",
            "Mean Loss 0.887351809690396\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 160\n",
            "Mean Loss 0.8858094194697009\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 161\n",
            "Mean Loss 0.8852718982983518\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 162\n",
            "Mean Loss 0.8849925420902394\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 163\n",
            "Mean Loss 0.884748822974938\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 164\n",
            "Mean Loss 0.8845903445725087\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 165\n",
            "Mean Loss 0.8841534519085178\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 166\n",
            "Mean Loss 0.883944574881483\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 167\n",
            "Mean Loss 0.8837624468185283\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 168\n",
            "Mean Loss 0.8834376663521484\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 169\n",
            "Mean Loss 0.8829017776857924\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 170\n",
            "Mean Loss 0.8825140743067971\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 171\n",
            "Mean Loss 0.8823851159325352\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 172\n",
            "Mean Loss 0.8818691189366358\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 173\n",
            "Mean Loss 0.8812263178880568\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 174\n",
            "Mean Loss 0.8804102719382003\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 175\n",
            "Mean Loss 0.8795887800278487\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 176\n",
            "Mean Loss 0.8784917183220387\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 177\n",
            "Mean Loss 0.8767587556331246\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 178\n",
            "Mean Loss 0.8749873744392836\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 179\n",
            "Mean Loss 0.8719140081493942\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 180\n",
            "Mean Loss 0.8677369574153865\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 181\n",
            "Mean Loss 0.861242307557\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 182\n",
            "Mean Loss 0.851243960360686\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 183\n",
            "Mean Loss 0.8344750772747729\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 184\n",
            "Mean Loss 0.8120521428408446\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 185\n",
            "Mean Loss 0.7445946405124333\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 186\n",
            "Mean Loss 0.7920804309348265\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 187\n",
            "Mean Loss 0.7959646314934448\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 188\n",
            "Mean Loss 0.7947988151400177\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 189\n",
            "Mean Loss 0.7761886958722715\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 190\n",
            "Mean Loss 0.7791314179560652\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 191\n",
            "Mean Loss 0.7053609719430959\n",
            "Mean accuracy 0.9166666666666666\n",
            "Epoch 192\n",
            "Mean Loss 0.7122628590850918\n",
            "Mean accuracy 0.9166666666666666\n",
            "Epoch 193\n",
            "Mean Loss 0.681331445083574\n",
            "Mean accuracy 0.9166666666666666\n",
            "Epoch 194\n",
            "Mean Loss 0.7354389527743613\n",
            "Mean accuracy 0.9166666666666666\n",
            "Epoch 195\n",
            "Mean Loss 0.8431123287451489\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 196\n",
            "Mean Loss 0.7254348997440603\n",
            "Mean accuracy 0.6898148148148148\n",
            "Epoch 197\n",
            "Mean Loss 0.7297849050136628\n",
            "Mean accuracy 0.9166666666666666\n",
            "Epoch 198\n",
            "Mean Loss 0.5448864578372903\n",
            "Mean accuracy 0.9166666666666666\n",
            "Epoch 199\n",
            "Mean Loss 0.5190856799621273\n",
            "Mean accuracy 0.9166666666666666\n",
            "Epoch 200\n",
            "Mean Loss 0.5002459220726181\n",
            "Mean accuracy 0.9166666666666666\n"
          ]
        }
      ],
      "source": [
        "evaluate()\n",
        "for i in range(200):\n",
        "  print(\"Epoch\", i+1)\n",
        "  train()\n",
        "  evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qwe0SoFzRoRe"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"exercise_classifier.pt\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.12 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "57c1e27abe1595a30b9f0267591cebadda67ffb3233e3fd93e2c2444299a025a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
